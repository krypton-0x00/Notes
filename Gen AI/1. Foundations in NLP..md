### Making Text Understandable to Computers
-> Text Preprocessing.
-> Tokenization
-> Embeddings

#### Text Processing
**Preparing the text by:**
- Lowercasing
- Removing punctuation
- Removing stop words
- Stemming / Lemmatization.

![[Pasted image 20250119081451.png]]

**Stemming vs Lemmatization***
![[Pasted image 20250119081615.png]]

Stemming is harsh, Lemmatization is flexible 

#### Tokenization
Break text into smaller parts:
- word tokenization
- sub word tokenization
![[Pasted image 20250119082237.png]]

#### Embeddings
Represents words as numbers so computers can understand similarities:
- Turns words into vectors.
- Similar words have similar vectors.
eg: king and queen have similar vectors.

![[Pasted image 20250119083053.png]]
![[Pasted image 20250119083322.png]]
Algos: Word2vec py.
